---
layout: standard
title: Meeting the Standard
permalink: /standard/meeting-standard/
lede: How services are assessed
---
All services within the [scope of the Digital Service Standard](/standard/scope-of-standard/) (the Standard) are assessed against the Standard.

Assessing against the Standard ensures that we build high-quality government services that meet user needs.

## Assurance framework

### Services built or redesigned after 6 May 2016

Services will go through an independent assessment process that is either:

* <a id="dtaled"></a>a Digital Transformation Agency (DTA) led assessment – for all new and redesigned transactional services that have / will have greater than 50,000 transactions per year (high volume)
* an agency self-assessment
  * for all new and redesigned transactional services that have / will have less than 50,000 transactions per year
  * any information service.

### Existing services

For high volume transactional services that were operating or started redesign before 6 May 2016 you can check how well your service is performing against the Standard by inviting the DTA to do a review by emailing [standard@digital.gov.au](mailto:standard@digital.gov.au).

The DTA reviews of existing services will be similar to the [staged assessment process](/standard/meeting-standard/staged-assessment-process/). The DTA will arrange a team of experts to review your service against the Standard and give you recommendations on how you can make your service simpler, clearer and faster.

## Assessment processes

Your service will be assessed as it moves through the stages of the [service design and delivery process](/standard/service-design-and-delivery-process/). Depending on the kind of service this can happen as:

* [in-flight assessment](#inflight) - regular check-ins by a team of experts as you design and develop your service
* [staged assessment](/standard/meeting-standard/staged-assessment-process/) - assessments at 3 points in the service design and delivery process.

All DTA-led assessments on high volume transactional services will be in-flight assessments.

For agency self-assessments on all other services, you may choose to use the [in-flight process](#inflight) or the [staged assessment process](/standard/meeting-standard/staged-assessment-process/).

### Assessment changes during the service design and delivery stages

As your service moves through the [service design and delivery process](/standard/service-design-and-delivery-process/) the assessments become more comprehensive:

1. By the end of **Alpha stage** the service must have passed criteria 1 to 3 of the Standard and show progress against other criteria.
2. During the **Beta stage** the service must have passed all criteria of the Standard before you can release the product as a public beta.
3. At the end of the Beta stage, before you move to the **Live stage** you must have passed all criteria of the Standard.

If your service doesn’t meet the Standard it shouldn’t progress through the service design and delivery process until the relevant criteria of the Standard have been met.

### Assessment team

In-flight and staged assessments are conducted by a team that is independent of the [digital delivery team](/standard/design-guides/the-team/). The assessors are experts and practitioners in their field, and will usually be working on their own projects. This ensures that the knowledge and skills of the assessors are up to date.

The assessing team will generally include 3 assessors (one of which is nominated as the lead assessor) who represent:

* service design/user research
* technology
* agile delivery.

You can contact the DTA Standard team by emailing [standard@digital.gov.au](mailto:standard@digital.gov.au) for help on setting up an assessment team. Assessment teams are assigned during the mobilisation / kick-off stage, before the service enters Discovery, so that assessors are involved as early as possible in the project.

### <a id="inflight"></a>In-flight assessment process

The in-flight assessment focuses on continuous improvement and learning for the digital delivery team and the assessor team. It is less about regular tests and more about a mentoring relationship between both teams. Transitioning between the stages should be a point of celebration for both teams.

#### Meet with assessors regularly

Your digital delivery team will meet with the assessment team for at least 1 hour once a week during the Discovery, Alpha and Beta stages, until the service is launched as a public beta.

As the service moves towards the Live stage, the assessment and digital delivery teams may consider moving check-ins to longer intervals, but they should happen at least monthly.

If you are moving from staged assessment to in-flight, you might spend more time on the first check-in meeting to help the assessment team understand the product.

Your meetings should focus on progress against the Digital Service Standard, documented using a red, amber, green (RAG) kanban. You can use the [Digital Service Standard kanban poster (PDF 117 KB)](/Digital Service Standard - Kanban poster-WCAG.pdf) to help with this. Both teams work together to make sure the service meets all of the requirements for each criterion in the current stage.

The assessors document what the teams have done well since the last meeting and make recommendations for the next period of work. The assessors will also provide guidance to the team to help them continue to work more closely to the Standard and achieve successful outcomes for their project.

Comments and RAG ratings should be complete by the end of the in-flight session and available for weekly reporting to stakeholders. Assessors will use a spreadsheet to track your progress and record recommendations.

#### Agree to transition to the next stage

Both the assessors and the service team agree when the requirements of the stage have been fulfilled and when the service can progress. This may be indicated by relevant criteria being assessed as green in the RAG rating.

This transition will also be documented by a brief report highlighting the achievements and learnings of the previous stage.

If your service is a high volume transactional service, or you started designing/redesigning it after 6 May 2016, the Alpha, Beta and Live reports will be provided to the DTA for publishing. Services moving to public beta will also start reporting on the [Performance Dashboard](/what-we-do/platforms/performance/).

## Report publicly

At each of the 3 stages - Alpha, Beta, Live - the assessment team produces a report. We publish [assessment reports](/standard/assessments/) on the DTA website to:  

* contribute to a government-wide repository of better practice
* provide a resource for other teams redesigning or building services
* provide an easy way to share ideas within and across departments
* help agency teams to promote their good work
* provide an audit trail of progress to support the assessment process
* demonstrate commitment to the Standard and increasing transparency of government to the community.

## Maintain your service

Launching your service is only the beginning. Once a service goes live you will need to keep listening to your users to make sure it continues to meet their needs and you are still following the Standard.

You will need to continue to update and improve the service on the basis of user feedback, performance data, changes to best practice and service demand.

You will also need to check:

* high levels of user satisfaction and transaction completion are maintained
* cost per transaction is decreasing
* digital take-up is increasing, and assisted digital support is provided to the people who really need it
* progress against other service-specific measurements you identified during the service development.

## Further reading

* DTA blog: [Moving to an agile ‘in-flight’ assessment model](/blog/in-flight-assessment-model/)

**Last updated:** 19 April 2017
