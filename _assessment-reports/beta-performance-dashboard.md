---
layout: assessment-report
title: Performance dashboard				
breadcrumb: Performance dashboard		
permalink:	/standard/assessments/performance-dashboard-beta/
report-excerpt: The performance dashboard will measure the performance of government services against the key performance indicators (KPIs) defined in the Digital Service Standard and other service related metrics and report them publicly.
department: Digital Transformation Agency			
date: 2016-10-20
result: Pass
assessment-stage: Beta
self-assessment: false
lead-assessor: Gary Broadbent
product-manager: Mariam Ibraheim
criteria-1-result: Pass
criteria-2-result: Pass
criteria-3-result: Pass
criteria-4-result: Pass
criteria-5-result: Pass
criteria-6-result: Pass
criteria-7-result: Pass
criteria-8-result: Pass
criteria-9-result: Pass
criteria-10-result: Pass
criteria-11-result: Pass
criteria-12-result: Pass
criteria-13-result: Pass
---
Based on the information provided throughout the inflight assessment the Performance Dashboard has met the criteria for the Beta assessment. 

## Areas of good performance

### Criterion 1 - Understand user needs

User research and testing is informing the product roadmap and feature deployment. We saw evidence of the continuing research and testing plans, user feedback and analytics. This informed decisions about new features such as an automated self-service area.

### Criterion 2 - Have a multidisciplinary team

The team is fully staffed with a multidisciplinary team. We saw a rapid improvement on product vision and improving service transparency. The team is working well together. All members understanding their roles, responsibilities and team processes. 

### Criterion 3 - Agile and user-centred process

The team showed ability to adapt and prioritise work to meet key milestones using the Service design and delivery process. This was despite changes in team size and structure between Alpha and Beta.

The team collectively established a test plan and identified two key user cohorts. They are developing tools to meet identified user needs.

Prototypes are tested with users, and the team showed how they analyse learnings and feedback for their next sprint plan. This process directly informs interations and improvements.

### Criteria 4 - Understand tools and systems

The product is now deployed on cloud.gov.au and tracking real time performance statistics. The team is using Sensu for monitoring the service.

The team is improving the Dashboard backend with a Ruby on Rail application and database.

A Data Information Plan is used to document, manage and address the risks associated with privacy, security and recordkeeping.

### Criterion 6 & 9 - Consistent and responsive design & make it accessible

The team is continually iterating to improve accessibility. They discussed accessibility needs in depth, demonstrating how issues are identified and resolved. Testing is underway for all new features to become compliant to WCAG 2.0 level AA.

The team continue to support development of the Digital Transformation Agency's (DTA) UI Kit. They have shared relevant research, technology, design feedback and recommendations.

The platform has taken a mobile first approach with the UI-Kit framework using common design patterns. The team has deployed BrowserStack to support test plans.

### Criterion 8 - Make source code open

The team are building in the open and have shared their code repositories. The team is facilitating easy access to the Dashboard API. 

Users of the Performance Dashboard have access to all dashboard data, which is non-identifiable.

### Criterion 13 - Encourage everyone to use the digital service

The team is actively engaging with whole of government and seeking ongoing opportunities. Activities have included hosting events and writing blog posts.

## Recommendations

We recommend that the team considers the following as they move through their Public Beta.

### Criterion 2 - Have a multidisciplinary team

We encourage the team to look at options to ensure that all remote team members can participate in all team discussions.

### Criterion 3 - Agile and user-centred process

The team should discuss how to better prioritise work; whether it originates as a bug, user feedback, browser support or accessibility improvement. This will support the user-centred design process to improve velocity.

The team should revisit its analysis, learning and feedback from users to better understand how the available data is presented in the platform. They should review improvements to KPIs and consider how they may scale and apply to other services joining the dashboard.

### Criterion 4 - Understand tools and systems

The team should consider implementing QA Fire to enable a continuous delivery model using feature branches in Github.

### Criterion 9 - Make it accessible

The team should extend its research and usability testing to include a broader range of users outside the DTA.

The team should prioritise and enhance their usability testing plan. The plan will help them further iterate the service to meet accessibility standards and improve useability for people regardless of their ability and environment.

### Criterion 13 - Encourage everyone to use the digital service

The team should find new ways of encouraging larger government agencies to get involved. Their communication strategy should demonstrate an understanding of dependencies and opportunities to expand engagement. 

They could consider improved collaboration with the internal communication team at the DTA to help identify new services for the Performance Dashboard.

## Follow the service

- Visit the [Performance Dashboard](https://dashboard.gov.au/)
- View [system performance](http://status.cloud.gov.au/)

### Code

This service's code is hosted in open source GitHub repositories:

- github.com/AusDTO/dto-dashboard
- [https://github.com/AusDTO/cg-dashboard](https://github.com/AusDTO/cg-dashboard)

### Metrics

Performance Dashboard:
[https://dashboard.gov.au/dashboards/8-performance-dashboard-dashboard](https://dashboard.gov.au/dashboards/8-performance-dashboard-dashboard)

